prompt_template: "Provide complete structured response for a formal software audit. Given this Context:\n'{context}'\n\nPlease provide a very detailed, accurate, and insightful Response to this Instruction and include your reasoning step by step. \n### Instruction:\n{query}\n### Response:"
inference_model:
  model_import_path: "ctransformers.AutoModelForCausalLM"
  model_inference_function: "from_pretrained"
  model_params:
    model_path: "TheBloke/TinyLlama-1.1B-python-v0.1-GGUF"
    model_file: "tinyllama-1.1b-python-v0.1.Q4_K_M.gguf"
    model_type: "llama"
    #model_path: "TheBloke/WizardCoder-Python-13B-V1.0-GGUF"
    #model_file: "wizardcoder-python-13b-v1.0.Q5_K_S.gguf"
    #model_type: "llama"
    local_files_only: false
    #model_path: "./models/wizardcoder-python-13b-v1.0.Q4_0.gguf"
    #model_type: "llama"
    #local_files_only: true
    #model_path: "TheBloke/WizardCoder-Guanaco-15B-V1.1-GGML"
    #model_type: "gpt_bigcode"
    #local_files_only: false
    #model_path: "TheBloke/Starcoderplus-Guanaco-GPT4-15B-V1.0-GGML"
    #model_type: "gpt_bigcode"
    #local_files_only: false
    #model_path: "TheBloke/Octocoder-GGML"
    #model_type: "gpt_bigcode"
    #local_files_only: false  

    ## MODEL CONFIGURATION PARAMETERS (GPU 4090 - 24GB VRAM, CPU 5950x - 32 threads, 64GB RAM)
    #avx2 and gpu_layers are not compatible 
    lib: "avx2"
    threads: 16
    batch_size: 25
    context_length: 20480
    max_new_tokens: 20480
    #gpu_layers: 100
    reset: true